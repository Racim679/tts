# ğŸ“š Guide: GitHub + Hugging Face pour Darija TTS

Ce guide explique comment utiliser votre projet avec GitHub (code) et Hugging Face (audio).

## ğŸ¯ Architecture

```
GitHub (arable-tts)           Hugging Face (darija-dataset)
â”œâ”€â”€ Scripts Python            â”œâ”€â”€ wavs/
â”œâ”€â”€ Notebooks                 â”‚   â”œâ”€â”€ audio001.wav
â”œâ”€â”€ Configs                   â”‚   â”œâ”€â”€ audio002.wav
â””â”€â”€ README.md                 â”‚   â””â”€â”€ ...
                              â”œâ”€â”€ metadata_train.csv
                              â”œâ”€â”€ metadata_eval.csv
                              â””â”€â”€ metadata.json
```

---

## ğŸ“¤ Ã‰TAPE 1: Upload du Dataset sur Hugging Face

### 1.1. CrÃ©er un dataset sur Hugging Face

1. Allez sur https://huggingface.co/new-dataset
2. CrÃ©ez un nouveau dataset (exemple: `votre-username/darija-dataset`)
3. CrÃ©ez un token d'accÃ¨s:
   - https://huggingface.co/settings/tokens
   - CrÃ©ez un token avec permission **write**

### 1.2. Installer les dÃ©pendances

```bash
pip install huggingface_hub
```

### 1.3. Upload automatique

Utilisez le script fourni:

```bash
python upload_to_huggingface.py \
  --token VOTRE_TOKEN_HF \
  --repo votre-username/darija-dataset \
  --dataset-path dataset_training
```

**Ou avec variable d'environnement:**

```bash
# Windows PowerShell
$env:HF_TOKEN="votre_token"
python upload_to_huggingface.py --repo votre-username/darija-dataset

# Linux/Mac
export HF_TOKEN="votre_token"
python upload_to_huggingface.py --repo votre-username/darija-dataset
```

### 1.4. VÃ©rification

Allez sur `https://huggingface.co/datasets/votre-username/darija-dataset`

Vous devriez voir:
- âœ… Dossier `wavs/` avec tous vos fichiers audio
- âœ… `metadata_train.csv`
- âœ… `metadata_eval.csv`
- âœ… `metadata.json`

---

## ğŸ™ Ã‰TAPE 2: Mettre le Code sur GitHub

### 2.1. Initialiser Git (si pas dÃ©jÃ  fait)

```bash
cd "C:\Users\Racim\Desktop\arable tts"
git init
```

### 2.2. VÃ©rifier le .gitignore

Le fichier `.gitignore` est dÃ©jÃ  configurÃ© pour exclure:
- âŒ Fichiers audio (*.wav, *.mp3, *.m4a)
- âŒ ModÃ¨les (*.pth, *.pt)
- âŒ Dossier `dataset_training/wavs/`
- âŒ Token Hugging Face

### 2.3. CrÃ©er un repo sur GitHub

1. Allez sur https://github.com/new
2. CrÃ©ez un nouveau repo (exemple: `arable-tts`)
3. **NE PAS** initialiser avec README (vous en avez dÃ©jÃ  un)

### 2.4. Push vers GitHub

```bash
# Ajouter tous les fichiers
git add .

# CrÃ©er le premier commit
git commit -m "Initial commit: Darija TTS avec GitHub + HuggingFace"

# Lier au repo distant
git remote add origin https://github.com/votre-username/arable-tts.git

# Push
git branch -M main
git push -u origin main
```

---

## ğŸš€ Ã‰TAPE 3: Utiliser dans Google Colab

### 3.1. Ouvrir le notebook

1. Uploadez `COLAB_NOTEBOOK_FINAL.ipynb` sur votre Google Drive
2. Ouvrez-le avec Google Colab
3. **OU** crÃ©ez un nouveau notebook et copiez les cellules

### 3.2. Configurer les repos

Dans la premiÃ¨re cellule de configuration:

```python
GITHUB_REPO = "votre-username/arable-tts"
HF_DATASET_REPO = "votre-username/darija-dataset"
HF_TOKEN = ""  # Laissez vide si dataset public
```

### 3.3. ExÃ©cuter le notebook

1. **Runtime â†’ Change runtime type â†’ GPU (T4)**
2. ExÃ©cutez toutes les cellules dans l'ordre
3. Le dataset sera tÃ©lÃ©chargÃ© automatiquement depuis Hugging Face

---

## ğŸ”„ Workflow Complet

### Workflow quotidien

```
1. Modifier le code en local
   â†“
2. git add . && git commit -m "description"
   â†“
3. git push
   â†“
4. Dans Colab: !git pull (si dÃ©jÃ  clonÃ©)
   â†“
5. Training avec donnÃ©es depuis HuggingFace
```

### Ajouter de nouveaux fichiers audio

```bash
# 1. Ajouter les fichiers dans dataset_training/wavs/
# 2. Mettre Ã  jour metadata.json

# 3. Re-upload sur HuggingFace
python upload_to_huggingface.py \
  --token VOTRE_TOKEN \
  --repo votre-username/darija-dataset
```

---

## ğŸ“¦ Scripts Disponibles

### `upload_to_huggingface.py`

Upload le dataset audio sur Hugging Face.

```bash
python upload_to_huggingface.py --token TOKEN --repo username/dataset
```

### `download_from_huggingface.py`

TÃ©lÃ©charge le dataset localement (pour tests).

```bash
python download_from_huggingface.py --repo username/dataset --output dataset_training
```

---

## ğŸ’¡ Avantages de cette Architecture

| Aspect | GitHub | Hugging Face |
|--------|--------|--------------|
| **Code Python** | âœ… | âŒ |
| **Notebooks** | âœ… | âŒ |
| **Configs** | âœ… | âŒ |
| **Fichiers audio** | âŒ | âœ… |
| **MÃ©tadonnÃ©es CSV** | âŒ | âœ… |
| **ModÃ¨les entraÃ®nÃ©s** | âŒ | âœ… |

### Pourquoi?

- **GitHub**: LimitÃ© Ã  100MB par fichier â†’ Parfait pour code
- **Hugging Face**: OptimisÃ© pour gros datasets â†’ Parfait pour audio

---

## ğŸ› ï¸ DÃ©pannage

### Erreur: "Repository not found"

- VÃ©rifiez que le repo existe sur Hugging Face
- VÃ©rifiez le nom du repo (format: `username/repo-name`)

### Erreur: "Authentication failed"

- VÃ©rifiez votre token Hugging Face
- Assurez-vous qu'il a les droits **write**

### Dataset privÃ©

Si votre dataset est privÃ©, ajoutez le token:

```python
HF_TOKEN = "hf_..." # Votre token
```

### Upload lent

L'upload peut prendre du temps selon:
- Nombre de fichiers audio
- Taille totale du dataset
- Vitesse de votre connexion

**Conseil:** Lancez l'upload et laissez tourner.

---

## ğŸ“ Checklist de Setup Complet

- [ ] Dataset uploadÃ© sur Hugging Face
- [ ] Code pushÃ© sur GitHub
- [ ] `.gitignore` configurÃ© (fichiers audio exclus)
- [ ] Notebook Colab testÃ© et fonctionnel
- [ ] Token HF crÃ©Ã© (si dataset privÃ©)
- [ ] README mis Ã  jour avec vos repos

---

## ğŸ”— Liens Utiles

- **Hugging Face Hub**: https://huggingface.co/docs/hub
- **Git Documentation**: https://git-scm.com/doc
- **Google Colab**: https://colab.research.google.com/

---

## ğŸ“ Partager avec un CollÃ¨gue

Votre collÃ¨gue doit juste:

1. Ouvrir le notebook Colab: `COLAB_NOTEBOOK_FINAL.ipynb`
2. Modifier la config avec vos repos GitHub/HuggingFace
3. ExÃ©cuter toutes les cellules
4. **C'est tout!** Pas de compression/dÃ©compression

---

## âœ¨ Prochaines Ã‰tapes

Une fois le training terminÃ©:

1. Sauvegarder les checkpoints sur Drive (cellule 10)
2. Optionnel: Upload le modÃ¨le entraÃ®nÃ© sur HuggingFace (cellule 11)
3. Utiliser `generate_finetuned_direct.py` pour la gÃ©nÃ©ration

---

**Bon training! ğŸš€**
