{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "93GFgKEWM0tF"
   },
   "source": [
    "# \ud83c\udfa4 Fine-Tuning StyleTTS2 pour le Darija\n",
    "\n",
    "## Version Optimis\u00e9e GitHub + HuggingFace (Setup rapide ~3 min)\n",
    "\n",
    "**Sources:**\n",
    "- \ud83d\udce6 **Code:** `github.com/VOTRE-USERNAME/arable-tts` \u2b05\ufe0f Remplacez par votre username GitHub\n",
    "- \ud83c\udfb5 **Audio:** `huggingface.co/datasets/VOTRE-USERNAME/darija-dataset` \u2b05\ufe0f Remplacez par votre repo HF\n",
    "\n",
    "### Pr\u00e9requis:\n",
    "- GPU: T4, V100, ou A100 (minimum 16GB VRAM)\n",
    "- Dataset d\u00e9j\u00e0 upload\u00e9 sur Hugging Face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TorA2qajM0tI"
   },
   "source": [
    "## \ud83d\udcdd Configuration - MODIFIEZ ICI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Udl9J2kXM0tI",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765374795380,
     "user_tz": -60,
     "elapsed": 91,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "9480e725-94ae-4476-8f46-79e002c936d5"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 GitHub: Racim679/tts\n"
     ]
    }
   ],
   "source": [
    "# \ud83d\udd27 CONFIGURATION - Modifiez ces valeurs\n",
    "GITHUB_REPO = \"Racim679/tts\"  # Votre repo GitHub\n",
    "\n",
    "print(f\"\u2705 GitHub: {GITHUB_REPO}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X38KZ0oPM0tJ"
   },
   "source": [
    "## 1. V\u00e9rification GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PHDX3MCgM0tJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765374806439,
     "user_tz": -60,
     "elapsed": 11057,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "bba07c7e-a437-4f0d-f7c5-234c4644557b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Wed Dec 10 13:53:17 2025       \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
      "|-----------------------------------------+------------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                        |               MIG M. |\n",
      "|=========================================+========================+======================|\n",
      "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
      "| N/A   70C    P8             11W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
      "|                                         |                        |                  N/A |\n",
      "+-----------------------------------------+------------------------+----------------------+\n",
      "                                                                                         \n",
      "+-----------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                              |\n",
      "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
      "|        ID   ID                                                               Usage      |\n",
      "|=========================================================================================|\n",
      "|  No running processes found                                                             |\n",
      "+-----------------------------------------------------------------------------------------+\n",
      "PyTorch: 2.9.0+cu126\n",
      "CUDA: True\n",
      "GPU: Tesla T4\n",
      "VRAM: 15.83 GB\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "import torch\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NrvdXR_dM0tJ"
   },
   "source": [
    "## 2. Installation des d\u00e9pendances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aIH2yS4sM0tJ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765374877719,
     "user_tz": -60,
     "elapsed": 71283,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "5e9b7871-2758-4f1d-ffb7-8dc02a3e5a3b"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m0.0/180.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m180.3/180.3 kB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m60.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m163.5/163.5 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m235.8/235.8 kB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m60.7/60.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m213.4/213.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m587.2/587.2 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for distance (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for docopt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for monotonic_align (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Selecting previously unselected package libpcaudio0:amd64.\n",
      "(Reading database ... 121713 files and directories currently installed.)\n",
      "Preparing to unpack .../libpcaudio0_1.1-6build2_amd64.deb ...\n",
      "Unpacking libpcaudio0:amd64 (1.1-6build2) ...\n",
      "Selecting previously unselected package libsonic0:amd64.\n",
      "Preparing to unpack .../libsonic0_0.2.0-11build1_amd64.deb ...\n",
      "Unpacking libsonic0:amd64 (0.2.0-11build1) ...\n",
      "Selecting previously unselected package espeak-ng-data:amd64.\n",
      "Preparing to unpack .../espeak-ng-data_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
      "Unpacking espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
      "Selecting previously unselected package libespeak-ng1:amd64.\n",
      "Preparing to unpack .../libespeak-ng1_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
      "Unpacking libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
      "Selecting previously unselected package espeak-ng.\n",
      "Preparing to unpack .../espeak-ng_1.50+dfsg-10ubuntu0.1_amd64.deb ...\n",
      "Unpacking espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
      "Setting up libpcaudio0:amd64 (1.1-6build2) ...\n",
      "Setting up libsonic0:amd64 (0.2.0-11build1) ...\n",
      "Setting up espeak-ng-data:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
      "Setting up libespeak-ng1:amd64 (1.50+dfsg-10ubuntu0.1) ...\n",
      "Setting up espeak-ng (1.50+dfsg-10ubuntu0.1) ...\n",
      "Processing triggers for man-db (2.10.2-1) ...\n",
      "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero_v2.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
      "\n",
      "/sbin/ldconfig.real: /usr/local/lib/libumf.so.1 is not a symbolic link\n",
      "\n",
      "\u2705 D\u00e9pendances install\u00e9es!\n"
     ]
    }
   ],
   "source": [
    "!pip install -q phonemizer==3.2.1 munch accelerate pydub nltk g2p_en num2words inflect unidecode pyyaml librosa scipy matplotlib soundfile\n",
    "!pip install -q torch torchaudio transformers einops einops-exts tqdm omegaconf huggingface_hub\n",
    "!pip install -q git+https://github.com/resemble-ai/monotonic_align.git\n",
    "!apt-get install -qq espeak-ng\n",
    "print(\"\u2705 D\u00e9pendances install\u00e9es!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_62eyJoM0tK"
   },
   "source": [
    "## 3. T\u00e9l\u00e9chargement du code (GitHub) et audio (HuggingFace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tweWK8jQM0tK",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765375560099,
     "user_tz": -60,
     "elapsed": 22659,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "8a698574-5121-44e5-c9b9-f31040f63fde"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2139\ufe0f StyleTTS2 d\u00e9j\u00e0 pr\u00e9sent\n",
      "\u2139\ufe0f Repo d\u00e9j\u00e0 pr\u00e9sent\n",
      "\ud83d\udce6 ZIP trouv\u00e9 sur Drive: /content/drive/MyDrive/darija_dataset.zip\n",
      "\u23f3 Extraction en cours... (\u00e7a peut prendre 1-2 min)\n",
      "\u2705 Donn\u00e9es extraites depuis Drive!\n",
      "\ud83c\udfb5 1052 fichiers audio d\u00e9tect\u00e9s\n",
      "\ud83d\udcc4 metadata_train.csv pr\u00e9sent\n",
      "\ud83d\udcc4 metadata_eval.csv pr\u00e9sent\n",
      "\ud83d\udcc4 metadata.json pr\u00e9sent\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from google.colab import drive\n",
    "\n",
    "# Monter Google Drive si n\u00e9cessaire\n",
    "if not os.path.exists('/content/drive'):\n",
    "    drive.mount('/content/drive')\n",
    "\n",
    "# \ud83d\ude80 CLONAGE DU PROJET (Source Unique: Racim679/tts)\n",
    "if not os.path.exists(\"/content/StyleTTS2\"):\n",
    "    print(\"\ud83d\udd04 Installation du code depuis Racim679/tts...\")\n",
    "    # On clone le repo principal dans un dossier temp\n",
    "    !git clone https://github.com/Racim679/tts.git /content/temp_repo\n",
    "    \n",
    "    # On d\u00e9place le dossier StyleTTS2 (le code) \u00e0 la racine de Colab\n",
    "    # C'est ce dossier qui contient tout le code unifi\u00e9\n",
    "    !mv /content/temp_repo/StyleTTS2 /content/StyleTTS2\n",
    "    \n",
    "    # Nettoyage\n",
    "    !rm -rf /content/temp_repo\n",
    "    print(\"\u2705 Code install\u00e9 avec succ\u00e8s !\")\n",
    "else:\n",
    "    print(\"\u2139\ufe0f Code d\u00e9j\u00e0 pr\u00e9sent\")\n",
    "\n",
    "# \ud83d\udce5 DONN\u00c9ES (Dataset)\n",
    "drive_zip_path = \"/content/drive/MyDrive/darija_dataset.zip\"\n",
    "local_dataset_path = \"/content/darija_dataset\"\n",
    "\n",
    "if os.path.exists(drive_zip_path):\n",
    "    print(f\"\ud83d\udce6 ZIP trouv\u00e9: {drive_zip_path}\")\n",
    "    if not os.path.exists(local_dataset_path):\n",
    "        print(\"\u23f3 Extraction...\")\n",
    "        !unzip -q {drive_zip_path} -d {local_dataset_path}\n",
    "        print(\"\u2705 Donn\u00e9es extraites !\")\n",
    "    else:\n",
    "        print(\"\u2139\ufe0f Donn\u00e9es d\u00e9j\u00e0 extraites\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f ZIP 'darija_dataset.zip' introuvable sur Drive !\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mFiHnpkdM0tK"
   },
   "source": [
    "## 4. T\u00e9l\u00e9chargement des mod\u00e8les pr\u00e9-entra\u00een\u00e9s"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "minimal_config = \"\"\"log_dir: \".\"\n",
    "save_freq: 1\n",
    "device: \"cuda\"\n",
    "epochs_1st: 200\n",
    "epochs_2nd: 800\n",
    "batch_size: 16\n",
    "max_len: 200\n",
    "\n",
    "model_params:\n",
    "  n_token: 178\n",
    "\n",
    "F0_path: \"Utils/JDC/bst.t7\"\n",
    "ASR_config: \"Utils/ASR/config.yml\"\n",
    "ASR_path: \"Utils/ASR/epoch_00080.pth\"\n",
    "\"\"\"\n",
    "\n",
    "with open('/content/StyleTTS2/Utils/ASR/config.yml', 'w') as f:\n",
    "    f.write(minimal_config)\n",
    "\n",
    "print(\"\u2705 Config ASR cr\u00e9\u00e9\")\n",
    "\n",
    "# V\u00e9rifier\n",
    "with open('/content/StyleTTS2/Utils/ASR/config.yml', 'r') as f:\n",
    "    print(f.read())\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O6JSdiOhTd7P",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765375588566,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "1eb952c0-3ea8-49a8-ce89-bdfb6d3f9541"
   },
   "execution_count": 20,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Config ASR cr\u00e9\u00e9\n",
      "log_dir: \".\"\n",
      "save_freq: 1\n",
      "device: \"cuda\"\n",
      "epochs_1st: 200\n",
      "epochs_2nd: 800\n",
      "batch_size: 16\n",
      "max_len: 200\n",
      "\n",
      "model_params:\n",
      "  n_token: 178\n",
      "\n",
      "F0_path: \"Utils/JDC/bst.t7\"\n",
      "ASR_config: \"Utils/ASR/config.yml\"\n",
      "ASR_path: \"Utils/ASR/epoch_00080.pth\"\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "# V\u00e9rifier que les fichiers de config existent et sont valides\n",
    "asr_config_path = \"/content/StyleTTS2/Utils/ASR/config.yml\"\n",
    "if os.path.exists(asr_config_path):\n",
    "    with open(asr_config_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        if len(content) > 0:\n",
    "             print(f\"\u2705 ASR config OK ({len(content)} bytes)\")\n",
    "        else:\n",
    "             print(\"\u274c ASR config est vide!\")\n",
    "else:\n",
    "      print(\"\u274c ASR config n'existe pas!\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q7K445rmR41m",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765375589982,
     "user_tz": -60,
     "elapsed": 10,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "95672b64-86fd-40be-a623-fa5ac2c5dd21"
   },
   "execution_count": 21,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 ASR config OK (233 bytes)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "3x8qrRc1Sch5"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m1tg2NNUM0tL"
   },
   "source": [
    "## 5. Patches PyTorch 2.6+ (CRITIQUE!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Etn23G6nM0tL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765375591713,
     "user_tz": -60,
     "elapsed": 15,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "83adfba3-f4be-4087-e726-a55bb3bf2c08"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2139\ufe0f OK: models.py\n",
      "\u2139\ufe0f OK: models.py\n",
      "\u2139\ufe0f OK: model.py\n",
      "\u2139\ufe0f OK: util.py\n",
      "\u2139\ufe0f OK: meldataset.py\n",
      "\n",
      "\u2705 Patches PyTorch appliqu\u00e9s!\n"
     ]
    }
   ],
   "source": [
    "import re, os\n",
    "\n",
    "files_to_patch = [\n",
    "    \"/content/StyleTTS2/models.py\",\n",
    "    \"/content/StyleTTS2/Utils/ASR/models.py\",\n",
    "    \"/content/StyleTTS2/Utils/JDC/model.py\",\n",
    "    \"/content/StyleTTS2/Utils/PLBERT/util.py\",\n",
    "    \"/content/StyleTTS2/meldataset.py\",\n",
    "]\n",
    "\n",
    "for filepath in files_to_patch:\n",
    "    if os.path.exists(filepath):\n",
    "        with open(filepath, 'r') as f:\n",
    "            content = f.read()\n",
    "        if 'torch.load' in content and 'weights_only' not in content:\n",
    "            new_content = re.sub(r'torch\\.load\\(([^)]+)\\)', r'torch.load(\\1, weights_only=False)', content)\n",
    "            with open(filepath, 'w') as f:\n",
    "                f.write(new_content)\n",
    "            print(f\"\u2705 Patched: {os.path.basename(filepath)}\")\n",
    "        else:\n",
    "            print(f\"\u2139\ufe0f OK: {os.path.basename(filepath)}\")\n",
    "\n",
    "print(\"\\n\u2705 Patches PyTorch appliqu\u00e9s!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zyVaEZ-AM0tL"
   },
   "source": [
    "## 6. Pr\u00e9paration des donn\u00e9es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PioG1rvKM0tL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765375598581,
     "user_tz": -60,
     "elapsed": 4265,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "20a64227-1171-4391-dac9-2db1e3241410"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udd0d Recherche automatique des donn\u00e9es...\n",
      "\u2705 Source d\u00e9tect\u00e9e : /content/darija_dataset\n",
      "\ud83c\udfb5 Copie des fichiers audio vers StyleTTS2...\n",
      "\u2705 1052 fichiers audio pr\u00eats.\n",
      "\u2705 Train list : 1442 lignes\n",
      "\u2705 Val list : 76 lignes\n",
      "\n",
      "\ud83c\udf89 Pr\u00e9paration termin\u00e9e ! Vous pouvez lancer l'entra\u00eenement.\n"
     ]
    }
   ],
   "source": [
    "import json, random, os, shutil\n",
    "\n",
    "print(\"\ud83d\udd0d Recherche automatique des donn\u00e9es...\")\n",
    "\n",
    "# 1. Identifier o\u00f9 sont les fichiers (supporte extraction racine ou dans un sous-dossier)\n",
    "source_dir = None\n",
    "possible_roots = [\"/content/dataset_darija\", \"/content/darija_dataset\", \"/content\"]\n",
    "\n",
    "for path in possible_roots:\n",
    "    if os.path.exists(os.path.join(path, \"metadata.json\")) and os.path.exists(os.path.join(path, \"wavs\")):\n",
    "        source_dir = path\n",
    "        break\n",
    "\n",
    "if not source_dir:\n",
    "    # Recherche recursive de secours si toujours pas trouv\u00e9\n",
    "    print(\"\u26a0\ufe0f Recherche approfondie en cours...\")\n",
    "    for root, dirs, files in os.walk(\"/content\"):\n",
    "        if \"metadata.json\" in files and \"wavs\" in dirs:\n",
    "            source_dir = root\n",
    "            print(f\"\u26a0\ufe0f Donn\u00e9es trouv\u00e9es dans un emplacement inhabituel : {source_dir}\")\n",
    "            break\n",
    "\n",
    "if not source_dir:\n",
    "    raise FileNotFoundError(\"\u274c Impossible de trouver 'metadata.json' et le dossier 'wavs'. V\u00e9rifiez l'\u00e9tape de d\u00e9compression du ZIP.\")\n",
    "\n",
    "print(f\"\u2705 Source d\u00e9tect\u00e9e : {source_dir}\")\n",
    "\n",
    "# 2. Pr\u00e9parer les dossiers de destination pour StyleTTS2\n",
    "os.makedirs(\"/content/StyleTTS2/Data\", exist_ok=True)\n",
    "os.makedirs(\"/content/StyleTTS2/wavs\", exist_ok=True)\n",
    "\n",
    "# 3. Copier les wavs\n",
    "print(\"\ud83c\udfb5 Copie des fichiers audio vers StyleTTS2...\")\n",
    "source_wavs = os.path.join(source_dir, \"wavs\")\n",
    "dest_wavs = \"/content/StyleTTS2/wavs\"\n",
    "\n",
    "wav_files = [f for f in os.listdir(source_wavs) if f.endswith('.wav')]\n",
    "if not wav_files:\n",
    "    raise FileNotFoundError(f\"\u274c Aucun fichier .wav trouv\u00e9 dans {source_wavs}\")\n",
    "\n",
    "# Copie plus s\u00fbre\n",
    "count = 0\n",
    "for f in wav_files:\n",
    "    src = os.path.join(source_wavs, f)\n",
    "    dst = os.path.join(dest_wavs, f)\n",
    "    # On \u00e9vite de copier si c'est d\u00e9j\u00e0 le m\u00eame fichier (cas o\u00f9 source = dest)\n",
    "    if os.path.abspath(src) != os.path.abspath(dst):\n",
    "        shutil.copy(src, dst)\n",
    "    count += 1\n",
    "\n",
    "print(f\"\u2705 {count} fichiers audio pr\u00eats.\")\n",
    "\n",
    "# 4. G\u00e9n\u00e9rer les listes train/val\n",
    "metadata_path = os.path.join(source_dir, \"metadata.json\")\n",
    "with open(metadata_path, 'r', encoding='utf-8') as f:\n",
    "    metadata = json.load(f)\n",
    "\n",
    "random.seed(42)\n",
    "random.shuffle(metadata)\n",
    "\n",
    "# 95% Train, 5% Val\n",
    "split = int(len(metadata) * 0.95)\n",
    "train_data = metadata[:split]\n",
    "eval_data = metadata[split:]\n",
    "\n",
    "def write_list(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            # Nettoyage vital pour \u00e9viter les erreurs de format (retours \u00e0 la ligne, pipes)\n",
    "            text = item['text'].replace('\\n', ' ').replace('\\r', '').replace('|', '')\n",
    "            f.write(f\"{item['audio_file']}|{text}|0\\n\")\n",
    "\n",
    "write_list(train_data, '/content/StyleTTS2/Data/train_list.txt')\n",
    "write_list(eval_data, '/content/StyleTTS2/Data/val_list.txt')\n",
    "\n",
    "print(f\"\u2705 Train list : {len(train_data)} lignes\")\n",
    "print(f\"\u2705 Val list : {len(eval_data)} lignes\")\n",
    "print(\"\\n\ud83c\udf89 Pr\u00e9paration termin\u00e9e ! Vous pouvez lancer l'entra\u00eenement.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "syQyWnejM0tL"
   },
   "source": [
    "## 7. Configuration du training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oufoaImkM0tL",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765375604437,
     "user_tz": -60,
     "elapsed": 33,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "2485d5ac-56bf-4f74-a5d7-d2736cedbdd0"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Configuration cr\u00e9\u00e9e!\n"
     ]
    }
   ],
   "source": [
    "import yaml, os\n",
    "\n",
    "config = {\n",
    "    'log_dir': 'Models/Darija',\n",
    "    'save_freq': 10,\n",
    "    'device': 'cuda',\n",
    "    'epochs_1st': 0,\n",
    "    'epochs_2nd': 80,\n",
    "    'batch_size': 32,\n",
    "    'max_len': 400,\n",
    "    'pretrained_model': 'Models/LibriTTS/epochs_2nd_00020.pth',\n",
    "    'data_params': {\n",
    "        'train_data': 'Data/train_list.txt',\n",
    "        'num_workers': 8,\n",
    "        'val_data': 'Data/val_list.txt',\n",
    "        'root_path': '',\n",
    "        'OOD_data': 'Data/OOD_texts.txt',\n",
    "        'min_length': 50,\n",
    "        'sample_rate': 24000,\n",
    "    },\n",
    "    'preprocess_params': {\n",
    "        'sr': 24000,\n",
    "        'spect_params': {\n",
    "            'n_fft': 2048,\n",
    "            'win_length': 1200,\n",
    "            'hop_length': 300,\n",
    "            'n_mels': 80\n",
    "        }\n",
    "    },\n",
    "    'model_params': {\n",
    "        'multispeaker': False,\n",
    "        'dim_in': 64,\n",
    "        'hidden_dim': 512,\n",
    "        'max_conv_dim': 512,\n",
    "        'n_layer': 3,\n",
    "        'n_mels': 80,\n",
    "        'n_token': 178,\n",
    "        'max_dur': 50,\n",
    "        'style_dim': 128,\n",
    "        'dropout': 0.2,\n",
    "        'decoder': {\n",
    "            'type': 'istftnet',\n",
    "            'resblock_kernel_sizes': [3, 7, 11],\n",
    "            'upsample_rates': [10, 6],\n",
    "            'upsample_initial_channel': 512,\n",
    "            'resblock_dilation_sizes': [[1, 3, 5], [1, 3, 5], [1, 3, 5]],\n",
    "            'upsample_kernel_sizes': [20, 12],\n",
    "            'gen_istft_n_fft': 20,\n",
    "            'gen_istft_hop_size': 5\n",
    "        },\n",
    "        'slm': {\n",
    "            'model': 'microsoft/wavlm-base-plus',\n",
    "            'sr': 16000,\n",
    "            'hidden': 768,\n",
    "            'nlayers': 13,\n",
    "            'initial_channel': 64\n",
    "        },\n",
    "        'diffusion': {\n",
    "            'embedding_mask_proba': 0.1,\n",
    "            'transformer': {\n",
    "                'num_layers': 3,\n",
    "                'num_heads': 8,\n",
    "                'head_features': 64,\n",
    "                'multiplier': 2\n",
    "            },\n",
    "            'dist': {\n",
    "                'sigma_data': 0.2,\n",
    "                'estimate_sigma_data': True,\n",
    "                'mean': -3.0,\n",
    "                'std': 1.0\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    'loss_params': {\n",
    "        'lambda_mel': 5.0,\n",
    "        'lambda_gen': 1.0,\n",
    "        'lambda_slm': 1.0,\n",
    "        'lambda_mono': 1.0,\n",
    "        'lambda_s2s': 1.0,\n",
    "        'lambda_F0': 1.0,\n",
    "        'lambda_norm': 1.0,\n",
    "        'lambda_dur': 1.0,\n",
    "        'lambda_ce': 20.0,\n",
    "        'lambda_sty': 1.0,\n",
    "        'lambda_diff': 1.0,\n",
    "        'diff_epoch': 20,\n",
    "        'joint_epoch': 40\n",
    "    },\n",
    "    'optimizer_params': {'lr': 0.0001},\n",
    "    'slmadv_params': {\n",
    "        'min_len': 400,\n",
    "        'max_len': 500,\n",
    "        'batch_percentage': 0.5,\n",
    "        'iter': 10,\n",
    "        'thresh': 5.0,\n",
    "        'scale': 0.01,\n",
    "        'sig': 1.5\n",
    "    },\n",
    "    'F0_path': 'Utils/JDC/bst.t7',\n",
    "    'ASR_config': 'Utils/ASR/config.yml',\n",
    "    'ASR_path': 'Utils/ASR/epoch_00080.pth',\n",
    "    'PLBERT_dir': 'Utils/PLBERT/',\n",
    "}\n",
    "\n",
    "os.makedirs('/content/StyleTTS2/Configs', exist_ok=True)\n",
    "with open('/content/StyleTTS2/Configs/config_darija_a100.yml', 'w') as f:\n",
    "    yaml.dump(config, f)\n",
    "\n",
    "print(\"\u2705 Configuration cr\u00e9\u00e9e!\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import requests\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"\ud83d\udce5 T\u00e9l\u00e9chargement depuis les VRAIS liens StyleTTS2...\\n\")\n",
    "\n",
    "downloads = [\n",
    "    (\"JDC\", \"https://github.com/yl4579/StyleTTS2/raw/main/Utils/JDC/bst.t7\",\n",
    "\"/content/StyleTTS2/Utils/JDC/bst.t7\"),\n",
    "    (\"ASR model\", \"https://github.com/yl4579/StyleTTS2/raw/main/Utils/ASR/epoch_00080.pth\",\n",
    "\"/content/StyleTTS2/Utils/ASR/epoch_00080.pth\"),\n",
    "      (\"PLBERT\", \"https://github.com/yl4579/StyleTTS2/raw/main/Utils/PLBERT/step_1000000.t7\",\n",
    "  \"/content/StyleTTS2/Utils/PLBERT/step_1000000.t7\"),\n",
    "    (\"PLBERT config\", \"https://github.com/yl4579/StyleTTS2/raw/main/Utils/PLBERT/config.yml\",\n",
    "\"/content/StyleTTS2/Utils/PLBERT/config.yml\"),\n",
    "]\n",
    "\n",
    "for name, url, path in downloads:\n",
    "    print(f\"\ud83d\udce5 {name}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=300, allow_redirects=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(path, 'wb') as f:\n",
    "                for chunk in response.iter_content(chunk_size=8192):\n",
    "                    f.write(chunk)\n",
    "\n",
    "            size = os.path.getsize(path) / 1024 / 1024\n",
    "            print(f\"\u2705 {size:.1f} MB\\n\")\n",
    "        else:\n",
    "            print(f\"\u274c HTTP {response.status_code}\\n\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c {str(e)}\\n\")\n",
    "    time.sleep(2)\n",
    "\n",
    "# V\u00e9rification finale\n",
    "print(\"\\n\ud83d\udd0d V\u00e9rification:\")\n",
    "for name, _, path in downloads:\n",
    "    if os.path.exists(path) and os.path.getsize(path) > 0:\n",
    "        print(f\"\u2705 {name}: OK\")\n",
    "    else:\n",
    "        print(f\"\u274c {name}: MANQUANT\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Aw36TE_oU7pD",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765375668801,
     "user_tz": -60,
     "elapsed": 9555,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "646ecd9d-8f23-46e8-9dae-e278bf784beb"
   },
   "execution_count": 26,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udce5 T\u00e9l\u00e9chargement depuis les VRAIS liens StyleTTS2...\n",
      "\n",
      "\ud83d\udce5 JDC...\n",
      "\u2705 20.1 MB\n",
      "\n",
      "\ud83d\udce5 ASR model...\n",
      "\u2705 90.2 MB\n",
      "\n",
      "\ud83d\udce5 PLBERT...\n",
      "\u2705 24.0 MB\n",
      "\n",
      "\ud83d\udce5 PLBERT config...\n",
      "\u2705 0.0 MB\n",
      "\n",
      "\n",
      "\ud83d\udd0d V\u00e9rification:\n",
      "\u2705 JDC: OK\n",
      "\u2705 ASR model: OK\n",
      "\u2705 PLBERT: OK\n",
      "\u2705 PLBERT config: OK\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    " AJOUT DES FICHIERS MANQUANTS LIBRYTTS ET CONFIG"
   ],
   "metadata": {
    "id": "OJ2mo_nNoeva"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import shutil\n",
    "import subprocess\n",
    "\n",
    "# 1. T\u00c9L\u00c9CHARGER LE MOD\u00c8LE MANQUANT (LibriTTS)\n",
    "print(\"\u23f3 T\u00e9l\u00e9chargement du mod\u00e8le LibriTTS (env. 300Mo)...\")\n",
    "model_dir = \"/content/StyleTTS2/Models/LibriTTS\"\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "model_url = \"https://huggingface.co/yl4579/StyleTTS2-LibriTTS/resolve/main/Models/LibriTTS/epochs_2nd_00020.pth\"\n",
    "model_path = os.path.join(model_dir, \"epochs_2nd_00020.pth\")\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    try:\n",
    "        subprocess.run([\"wget\", \"-O\", model_path, model_url], check=True)\n",
    "        print(\"\u2705 Mod\u00e8le LibriTTS t\u00e9l\u00e9charg\u00e9 !\")\n",
    "    except Exception as e:\n",
    "        print(f\"\u274c Erreur t\u00e9l\u00e9chargement : {e}\")\n",
    "else:\n",
    "    print(\"\u2139\ufe0f Mod\u00e8le LibriTTS d\u00e9j\u00e0 pr\u00e9sent.\")\n",
    "\n",
    "\n",
    "# 2. CORRIGER LE NOM DE LA CONFIG\n",
    "# Le script de v\u00e9rification cherche \"config_darija_ft.yml\" mais vous avez \"config_darija_a100.yml\"\n",
    "src_config = \"/content/StyleTTS2/Configs/config_darija_a100.yml\"\n",
    "dst_config = \"/content/StyleTTS2/Configs/config_darija_ft.yml\"\n",
    "\n",
    "if os.path.exists(src_config):\n",
    "    shutil.copy(src_config, dst_config)\n",
    "    print(f\"\u2705 Config copi\u00e9e et renomm\u00e9e : {dst_config}\")\n",
    "else:\n",
    "    print(f\"\u274c Config source introuvable : {src_config}\")\n",
    "\n",
    "print(\"\\n\ud83d\ude80 Tout devrait \u00eatre bon ! Relancez la cellule de v\u00e9rification.\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9UfEyzYodkk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765376431476,
     "user_tz": -60,
     "elapsed": 7406,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "230bf06e-b9c3-47e8-9879-94086af39502"
   },
   "execution_count": 28,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u23f3 T\u00e9l\u00e9chargement du mod\u00e8le LibriTTS (env. 300Mo)...\n",
      "\u2705 Mod\u00e8le LibriTTS t\u00e9l\u00e9charg\u00e9 !\n",
      "\u2705 Config copi\u00e9e et renomm\u00e9e : /content/StyleTTS2/Configs/config_darija_ft.yml\n",
      "\n",
      "\ud83d\ude80 Tout devrait \u00eatre bon ! Relancez la cellule de v\u00e9rification.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G8760Rp4M0tM"
   },
   "source": [
    "## 8. V\u00e9rification finale avant training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0uJOShB1M0tM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765376437564,
     "user_tz": -60,
     "elapsed": 14,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "7eaa66ff-4604-41be-e481-c16b81522ce2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Config\n",
      "\u2705 Audio (wavs)\n",
      "\u2705 Train list\n",
      "\u2705 Val list\n",
      "\u2705 Mod\u00e8le LibriTTS\n",
      "\u2705 JDC\n",
      "\u2705 ASR\n",
      "\u2705 PLBERT\n",
      "\n",
      "\ud83c\udfb5 1052 fichiers audio pr\u00eats\n",
      "\n",
      "\ud83c\udf89 TOUT EST PR\u00caT! Vous pouvez lancer le training!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "checks = {\n",
    "    \"Config\": \"/content/StyleTTS2/Configs/config_darija_ft.yml\",\n",
    "    \"Audio (wavs)\": \"/content/StyleTTS2/wavs\",\n",
    "    \"Train list\": \"/content/StyleTTS2/Data/train_list.txt\",\n",
    "    \"Val list\": \"/content/StyleTTS2/Data/val_list.txt\",\n",
    "    \"Mod\u00e8le LibriTTS\": \"/content/StyleTTS2/Models/LibriTTS/epochs_2nd_00020.pth\",\n",
    "    \"JDC\": \"/content/StyleTTS2/Utils/JDC/bst.t7\",\n",
    "    \"ASR\": \"/content/StyleTTS2/Utils/ASR/epoch_00080.pth\",\n",
    "    \"PLBERT\": \"/content/StyleTTS2/Utils/PLBERT/step_1000000.t7\",\n",
    "}\n",
    "\n",
    "all_ok = True\n",
    "for name, path in checks.items():\n",
    "    exists = os.path.exists(path)\n",
    "    status = \"\u2705\" if exists else \"\u274c\"\n",
    "    print(f\"{status} {name}\")\n",
    "    if not exists:\n",
    "        all_ok = False\n",
    "\n",
    "# Compter les fichiers\n",
    "if os.path.exists(\"/content/StyleTTS2/wavs\"):\n",
    "    wav_count = len([f for f in os.listdir(\"/content/StyleTTS2/wavs\") if f.endswith('.wav')])\n",
    "    print(f\"\\n\ud83c\udfb5 {wav_count} fichiers audio pr\u00eats\")\n",
    "\n",
    "if all_ok:\n",
    "    print(\"\\n\ud83c\udf89 TOUT EST PR\u00caT! Vous pouvez lancer le training!\")\n",
    "else:\n",
    "    print(\"\\n\u26a0\ufe0f Certains fichiers manquent. V\u00e9rifiez les \u00e9tapes pr\u00e9c\u00e9dentes.\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# V\u00e9rifier que tous les fichiers critiques existent\n",
    "import os\n",
    "\n",
    "files_to_check = {\n",
    "    \"Config principal\": \"/content/StyleTTS2/Configs/config_darija_ft.yml\",\n",
    "    \"Train list\": \"/content/StyleTTS2/Data/train_list.txt\",\n",
    "    \"Val list\": \"/content/StyleTTS2/Data/val_list.txt\",\n",
    "    \"Mod\u00e8le pretrained\": \"/content/StyleTTS2/Models/LibriTTS/epochs_2nd_00020.pth\",\n",
    "    \"JDC\": \"/content/StyleTTS2/Utils/JDC/bst.t7\",\n",
    "    \"ASR model\": \"/content/StyleTTS2/Utils/ASR/epoch_00080.pth\",\n",
    "    \"ASR config\": \"/content/StyleTTS2/Utils/ASR/config.yml\",\n",
    "    \"PLBERT\": \"/content/StyleTTS2/Utils/PLBERT/step_1000000.t7\",\n",
    "    \"PLBERT config\": \"/content/StyleTTS2/Utils/PLBERT/config.yml\"\n",
    "}\n",
    "\n",
    "print(\"\ud83d\udd0d V\u00e9rification des fichiers critiques:\\n\")\n",
    "all_ok = True\n",
    "\n",
    "for name, path in files_to_check.items():\n",
    "    exists = os.path.exists(path)\n",
    "    size = os.path.getsize(path) if exists else 0\n",
    "    status = \"\u2705\" if exists and size > 0 else \"\u274c\"\n",
    "\n",
    "    if exists and size > 0:\n",
    "        print(f\"{status} {name}: {size/1024/1024:.1f} MB\")\n",
    "    else:\n",
    "        print(f\"{status} {name}: VIDE ou MANQUANT\")\n",
    "\n",
    "    if not exists or size == 0:\n",
    "        all_ok = False\n",
    "\n",
    "print(f\"\\n{'\ud83c\udf89 Tout OK' if all_ok else '\u26a0\ufe0f Fichiers manquants d\u00e9tect\u00e9s'}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dOuOsRpzUmRb",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765376441049,
     "user_tz": -60,
     "elapsed": 10,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "213050b6-4e2f-41e9-ccd9-72bbc0021b05"
   },
   "execution_count": 30,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\ud83d\udd0d V\u00e9rification des fichiers critiques:\n",
      "\n",
      "\u2705 Config principal: 0.0 MB\n",
      "\u2705 Train list: 0.1 MB\n",
      "\u2705 Val list: 0.0 MB\n",
      "\u2705 Mod\u00e8le pretrained: 735.7 MB\n",
      "\u2705 JDC: 20.1 MB\n",
      "\u2705 ASR model: 90.2 MB\n",
      "\u2705 ASR config: 0.0 MB\n",
      "\u2705 PLBERT: 24.0 MB\n",
      "\u2705 PLBERT config: 0.0 MB\n",
      "\n",
      "\ud83c\udf89 Tout OK\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "asr_config_optimal = \"\"\"log_dir: \".\"\n",
    "save_freq: 5\n",
    "log_interval: 10\n",
    "\n",
    "epochs: 200\n",
    "batch_size: 16\n",
    "optimizer: \"AdamW\"\n",
    "\n",
    "data_params:\n",
    "  train_data: \"\"\n",
    "  val_data: \"\"\n",
    "  root_path: \"\"\n",
    "  OOD_data: \"\"\n",
    "  min_length: 50\n",
    "  max_length: 500\n",
    "\n",
    "loss_params:\n",
    "  lambda_mel: 5.0\n",
    "\n",
    "optimizer_params:\n",
    "  lr: 0.0001\n",
    "  betas: [0.9, 0.99]\n",
    "  weight_decay: 0.0\n",
    "\n",
    "model_params:\n",
    "  dim_in: 80\n",
    "  hidden_dim: 512\n",
    "  n_token: 178\n",
    "  token_embedding_dim: 512\n",
    "\n",
    "preprocess_params:\n",
    "  sr: 24000\n",
    "  spect_params:\n",
    "    n_fft: 2048\n",
    "    win_length: 1200\n",
    "    hop_length: 300\n",
    "    n_mels: 80\n",
    "\"\"\"\n",
    "\n",
    "# Sauvegarder\n",
    "with open('/content/StyleTTS2/Utils/ASR/config.yml', 'w') as f:\n",
    "    f.write(asr_config_optimal)\n",
    "\n",
    "print(\"\u2705 Config ASR optimal cr\u00e9\u00e9\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# PLBERT CONFIG\n",
    "# -----------------------------------------------------\n",
    "\n",
    "plbert_config = \"\"\"log_dir: \".\"\n",
    "save_freq: 2\n",
    "log_interval: 10\n",
    "device: \"cuda\"\n",
    "epochs: 50\n",
    "batch_size: 256\n",
    "\n",
    "model_params:\n",
    "  model_type: \"bert-base-uncased\"\n",
    "  hidden_size: 768\n",
    "  num_hidden_layers: 12\n",
    "  num_attention_heads: 12\n",
    "\n",
    "data_params:\n",
    "  train_data: \"\"\n",
    "  val_data: \"\"\n",
    "\n",
    "optimizer_params:\n",
    "  lr: 0.0001\n",
    "\"\"\"\n",
    "\n",
    "with open('/content/StyleTTS2/Utils/PLBERT/config.yml', 'w') as f:\n",
    "    f.write(plbert_config)\n",
    "\n",
    "print(\"\u2705 Config PLBERT cr\u00e9\u00e9\\n\")\n",
    "\n",
    "\n",
    "# -----------------------------------------------------\n",
    "# V\u00c9RIFICATION COMPL\u00c8TE\n",
    "# -----------------------------------------------------\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"\ud83d\udd0d V\u00c9RIFICATION FINALE COMPL\u00c8TE:\\n\")\n",
    "\n",
    "critical_files = {\n",
    "    \"Config Training\": \"/content/StyleTTS2/Configs/config_darija_ft.yml\",\n",
    "    \"Train List\": \"/content/StyleTTS2/Data/train_list.txt\",\n",
    "    \"Val List\": \"/content/StyleTTS2/Data/val_list.txt\",\n",
    "    \"Pretrained Model\": \"/content/StyleTTS2/Models/LibriTTS/epochs_2nd_00020.pth\",\n",
    "    \"LibriTTS Config\": \"/content/StyleTTS2/Models/LibriTTS/config.yml\",\n",
    "    \"JDC Model\": \"/content/StyleTTS2/Utils/JDC/bst.t7\",\n",
    "    \"ASR Model\": \"/content/StyleTTS2/Utils/ASR/epoch_00080.pth\",\n",
    "    \"ASR Config\": \"/content/StyleTTS2/Utils/ASR/config.yml\",\n",
    "    \"PLBERT Model\": \"/content/StyleTTS2/Utils/PLBERT/step_1000000.t7\",\n",
    "    \"PLBERT Config\": \"/content/StyleTTS2/Utils/PLBERT/config.yml\"\n",
    "}\n",
    "\n",
    "all_good = True\n",
    "\n",
    "for name, path in critical_files.items():\n",
    "    if os.path.exists(path):\n",
    "        size = os.path.getsize(path)\n",
    "        if size > 0:\n",
    "            size_mb = size / 1024 / 1024\n",
    "            print(f\"\u2705 {name}: {size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(f\"\u274c {name}: VIDE (0 bytes)\")\n",
    "            all_good = False\n",
    "    else:\n",
    "        print(f\"\u274c {name}: MANQUANT\")\n",
    "        all_good = False\n",
    "\n",
    "\n",
    "# Compter les fichiers audio\n",
    "wavs_dir = \"/content/StyleTTS2/wavs\"\n",
    "if os.path.exists(wavs_dir):\n",
    "    wav_files = [f for f in os.listdir(wavs_dir) if f.endswith('.wav')]\n",
    "    print(f\"\\n\ud83c\udfb5 Fichiers audio: {len(wav_files)} fichiers WAV\")\n",
    "else:\n",
    "    print(\"\\n\u274c Dossier wavs manquant!\")\n",
    "    all_good = False\n",
    "\n",
    "\n",
    "print(f\"\\n{'\ud83c\udf89 TOUT EST PR\u00caT POUR LE TRAINING!' if all_good else '\u26a0\ufe0f FICHIERS MANQUANTS - VOIR CI-DESSUS'}\")\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9K9biZ8OXZIf",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765376447101,
     "user_tz": -60,
     "elapsed": 12,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "60136e1a-35fa-4326-9bc1-be870929f1d9"
   },
   "execution_count": 31,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 Config ASR optimal cr\u00e9\u00e9\n",
      "\n",
      "\u2705 Config PLBERT cr\u00e9\u00e9\n",
      "\n",
      "\ud83d\udd0d V\u00c9RIFICATION FINALE COMPL\u00c8TE:\n",
      "\n",
      "\u2705 Config Training: 0.00 MB\n",
      "\u2705 Train List: 0.10 MB\n",
      "\u2705 Val List: 0.01 MB\n",
      "\u2705 Pretrained Model: 735.66 MB\n",
      "\u274c LibriTTS Config: MANQUANT\n",
      "\u2705 JDC Model: 20.06 MB\n",
      "\u2705 ASR Model: 90.17 MB\n",
      "\u2705 ASR Config: 0.00 MB\n",
      "\u2705 PLBERT Model: 24.02 MB\n",
      "\u2705 PLBERT Config: 0.00 MB\n",
      "\n",
      "\ud83c\udfb5 Fichiers audio: 1052 fichiers WAV\n",
      "\n",
      "\u26a0\ufe0f FICHIERS MANQUANTS - VOIR CI-DESSUS\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w3g8qaxeM0tM"
   },
   "source": [
    "## 9. \ud83d\ude80 Lancement du Fine-Tuning\n",
    "\n",
    "\u26a0\ufe0f **Dur\u00e9e estim\u00e9e: 8-12h sur GPU T4**\n",
    "\n",
    "Checkpoints sauvegard\u00e9s tous les 10 epochs dans `/content/StyleTTS2/Models/Darija/`"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Cell: Create PLBERT config that matches checkpoint\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "plbert_config = {\n",
    "    'model_params': {\n",
    "        'vocab_size': 178,           # Must match checkpoint (not 30000)\n",
    "        'hidden_size': 768,\n",
    "        'num_hidden_layers': 12,\n",
    "        'num_attention_heads': 12,\n",
    "        'intermediate_size': 2048,   # Must match checkpoint (not 16384)\n",
    "        'hidden_act': 'gelu',\n",
    "        'hidden_dropout_prob': 0.0,\n",
    "        'attention_probs_dropout_prob': 0.0,\n",
    "        'max_position_embeddings': 512,\n",
    "        'type_vocab_size': 2,\n",
    "        'initializer_range': 0.02,\n",
    "        'layer_norm_eps': 1e-12\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save PLBERT config\n",
    "plbert_config_path = '/content/StyleTTS2/Utils/PLBERT/config.yml'\n",
    "os.makedirs(os.path.dirname(plbert_config_path), exist_ok=True)\n",
    "\n",
    "with open(plbert_config_path, 'w') as f:\n",
    "    yaml.dump(plbert_config, f, default_flow_style=False)\n",
    "\n",
    "print(f\"\u2705 PLBERT config saved to {plbert_config_path}\")\n",
    "print(\"\\nConfig:\")\n",
    "with open(plbert_config_path, 'r') as f:\n",
    "     print(f.read())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uNpBVbcyYdJk",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765376584435,
     "user_tz": -60,
     "elapsed": 8,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "d94d3512-4127-481e-92bf-0195a0f8b6c8"
   },
   "execution_count": 32,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u2705 PLBERT config saved to /content/StyleTTS2/Utils/PLBERT/config.yml\n",
      "\n",
      "Config:\n",
      "model_params:\n",
      "  attention_probs_dropout_prob: 0.0\n",
      "  hidden_act: gelu\n",
      "  hidden_dropout_prob: 0.0\n",
      "  hidden_size: 768\n",
      "  initializer_range: 0.02\n",
      "  intermediate_size: 2048\n",
      "  layer_norm_eps: 1.0e-12\n",
      "  max_position_embeddings: 512\n",
      "  num_attention_heads: 12\n",
      "  num_hidden_layers: 12\n",
      "  type_vocab_size: 2\n",
      "  vocab_size: 178\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# \ud83d\ude80 LANCEMENT ROBUSTE DU TRAINING\n",
    "%cd /content/StyleTTS2\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# 1. Nettoyage m\u00e9moire\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 2. Forcer une config ASR propre (Correction Erreur \"dim_in\")\n",
    "asr_config_path = \"Utils/ASR/config.yml\"\n",
    "asr_clean_config = {\n",
    "    \"model_params\": {\n",
    "        \"input_dim\": 80,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"n_token\": 178,     # Votre valeur sp\u00e9cifique\n",
    "        \"n_layers\": 6,\n",
    "        \"token_embedding_dim\": 256 # Valeur standard compatible\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\ud83d\udee0\ufe0f R\u00e9paration de {asr_config_path}...\")\n",
    "os.makedirs(os.path.dirname(asr_config_path), exist_ok=True)\n",
    "with open(asr_config_path, 'w') as f:\n",
    "    yaml.dump(asr_clean_config, f, default_flow_style=False)\n",
    "print(\"\u2705 Config ASR r\u00e9g\u00e9n\u00e9r\u00e9e.\")\n",
    "\n",
    "# 3. Lancement\n",
    "print(\"\ud83c\udfc1 D\u00e9marrage de l'entra\u00eenement...\")\n",
    "!python train_finetune.py --config_path Configs/config_darija_ft.yml"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FlBbzDR1pakm",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765377399880,
     "user_tz": -60,
     "elapsed": 14925,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "79a5a7f9-1e92-459d-833c-afa831b16a8c"
   },
   "execution_count": 38,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/StyleTTS2\n",
      "\ud83d\udee0\ufe0f R\u00e9paration de Utils/ASR/config.yml...\n",
      "\u2705 Config ASR r\u00e9g\u00e9n\u00e9r\u00e9e.\n",
      "\ud83c\udfc1 D\u00e9marrage de l'entra\u00eenement...\n",
      "2025-12-10 14:36:30.045553: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765377390.080038   11392 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765377390.090436   11392 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765377390.114823   11392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765377390.114858   11392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765377390.114868   11392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765377390.114875   11392 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-10 14:36:30.122672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/StyleTTS2/train_finetune.py\", line 708, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1485, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1406, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1269, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 824, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/StyleTTS2/train_finetune.py\", line 124, in main\n",
      "    text_aligner = load_ASR_models(ASR_path, ASR_config)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/StyleTTS2/models.py\", line 609, in load_ASR_models\n",
      "    asr_model = _load_model(asr_model_config, ASR_MODEL_PATH)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/StyleTTS2/models.py\", line 605, in _load_model\n",
      "    model.load_state_dict(params)\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\", line 2629, in load_state_dict\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Error(s) in loading state_dict for ASRCNN:\n",
      "\tsize mismatch for asr_s2s.embedding.weight: copying a param with shape torch.Size([178, 512]) from checkpoint, the shape in current model is torch.Size([178, 256]).\n",
      "\tsize mismatch for asr_s2s.decoder_rnn.weight_ih: copying a param with shape torch.Size([512, 640]) from checkpoint, the shape in current model is torch.Size([512, 384]).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lOqC8cNWM0tM"
   },
   "source": [
    "## 10. Sauvegarde sur Google Drive (IMPORTANT!)"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# \ud83d\ude80 LANCEMENT ROBUSTE DU TRAINING (CORRECTION V2)\n",
    "%cd /content/StyleTTS2\n",
    "\n",
    "import torch\n",
    "import gc\n",
    "import os\n",
    "import yaml\n",
    "\n",
    "# 1. Nettoyage m\u00e9moire\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "# 2. Forcer une config ASR propre (CORRECTED)\n",
    "asr_config_path = \"Utils/ASR/config.yml\"\n",
    "asr_clean_config = {\n",
    "    \"model_params\": {\n",
    "        \"input_dim\": 80,\n",
    "        \"hidden_dim\": 256,\n",
    "        \"n_token\": 178,\n",
    "        \"n_layers\": 6,\n",
    "        \"token_embedding_dim\": 512  # <--- CORRECTION ICI (512 au lieu de 256)\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"\ud83d\udee0\ufe0f R\u00e9paration V2 de {asr_config_path}...\")\n",
    "os.makedirs(os.path.dirname(asr_config_path), exist_ok=True)\n",
    "with open(asr_config_path, 'w') as f:\n",
    "    yaml.dump(asr_clean_config, f, default_flow_style=False)\n",
    "print(\"\u2705 Config ASR r\u00e9g\u00e9n\u00e9r\u00e9e (Embedding 512).\")\n",
    "\n",
    "# 3. Lancement\n",
    "print(\"\ud83c\udfc1 D\u00e9marrage de l'entra\u00eenement...\")\n",
    "!python train_finetune.py --config_path Configs/config_darija_ft.yml"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rW410GgorY5w",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765377749292,
     "user_tz": -60,
     "elapsed": 15337,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "28aed3a4-0a4c-4937-da24-a11f395b3464"
   },
   "execution_count": 39,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/content/StyleTTS2\n",
      "\ud83d\udee0\ufe0f R\u00e9paration V2 de Utils/ASR/config.yml...\n",
      "\u2705 Config ASR r\u00e9g\u00e9n\u00e9r\u00e9e (Embedding 512).\n",
      "\ud83c\udfc1 D\u00e9marrage de l'entra\u00eenement...\n",
      "2025-12-10 14:42:18.937410: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765377738.969772   12850 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765377738.979734   12850 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1765377739.003803   12850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765377739.003838   12850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765377739.003846   12850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1765377739.003852   12850 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-12-10 14:42:19.010580: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Traceback (most recent call last):\n",
      "  File \"/content/StyleTTS2/train_finetune.py\", line 708, in <module>\n",
      "    main()\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1485, in __call__\n",
      "    return self.main(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1406, in main\n",
      "    rv = self.invoke(ctx)\n",
      "         ^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1269, in invoke\n",
      "    return ctx.invoke(self.callback, **ctx.params)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 824, in invoke\n",
      "    return callback(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/StyleTTS2/train_finetune.py\", line 132, in main\n",
      "    plbert = load_plbert(BERT_path)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/content/StyleTTS2/Utils/PLBERT/util.py\", line 30, in load_plbert\n",
      "    checkpoint = torch.load(log_dir + \"/step_\" + str(iters, weights_only=False) + \".t7\", map_location='cpu')\n",
      "                                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "TypeError: 'weights_only' is an invalid keyword argument for str()\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f6FWHVL0M0tM",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1765377177074,
     "user_tz": -60,
     "elapsed": 987,
     "user": {
      "displayName": "Racim Si Smail",
      "userId": "13880419371013186198"
     }
    },
    "outputId": "0ca865b4-bcda-40a3-8dee-058cc6f47ca1"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
      "\ud83d\udcbe Sauvegarde des checkpoints sur Google Drive...\n",
      "\u26a0\ufe0f Aucun checkpoint trouv\u00e9\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import shutil\n",
    "import os\n",
    "\n",
    "# Monter Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Cr\u00e9er le dossier de sauvegarde\n",
    "backup_dir = '/content/drive/MyDrive/darija_checkpoints'\n",
    "os.makedirs(backup_dir, exist_ok=True)\n",
    "\n",
    "# Copier les checkpoints\n",
    "print(\"\ud83d\udcbe Sauvegarde des checkpoints sur Google Drive...\")\n",
    "source_dir = '/content/StyleTTS2/Models/Darija'\n",
    "if os.path.exists(source_dir):\n",
    "    for item in os.listdir(source_dir):\n",
    "        source_path = os.path.join(source_dir, item)\n",
    "        dest_path = os.path.join(backup_dir, item)\n",
    "        if os.path.isfile(source_path):\n",
    "            shutil.copy2(source_path, dest_path)\n",
    "            print(f\"\u2705 {item}\")\n",
    "    print(\"\\n\ud83c\udf89 Checkpoints sauvegard\u00e9s sur Drive!\")\n",
    "else:\n",
    "    print(\"\u26a0\ufe0f Aucun checkpoint trouv\u00e9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNt5U3g1M0tN"
   },
   "source": [
    "## 11. Upload du mod\u00e8le entra\u00een\u00e9 sur Hugging Face (optionnel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PlRtbEXKM0tN"
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, login\n",
    "\n",
    "# Configuration\n",
    "HF_MODEL_REPO = \"VOTRE-USERNAME/darija-styletts2\"  # Changez ici\n",
    "HF_TOKEN_UPLOAD = \"\"  # Votre token HF avec droits write\n",
    "\n",
    "if HF_TOKEN_UPLOAD:\n",
    "    print(\"\ud83d\udd10 Connexion \u00e0 Hugging Face...\")\n",
    "    login(token=HF_TOKEN_UPLOAD)\n",
    "\n",
    "    api = HfApi()\n",
    "\n",
    "    # Cr\u00e9er le repo\n",
    "    api.create_repo(repo_id=HF_MODEL_REPO, repo_type=\"model\", exist_ok=True)\n",
    "\n",
    "    # Upload les checkpoints\n",
    "    print(f\"\ud83d\udce4 Upload vers {HF_MODEL_REPO}...\")\n",
    "    api.upload_folder(\n",
    "        folder_path=\"/content/StyleTTS2/Models/Darija\",\n",
    "        repo_id=HF_MODEL_REPO,\n",
    "        repo_type=\"model\"\n",
    "    )\n",
    "\n",
    "    print(f\"\u2705 Mod\u00e8le disponible sur: https://huggingface.co/{HF_MODEL_REPO}\")\n",
    "else:\n",
    "    print(\"\u2139\ufe0f Token HF non fourni, upload ignor\u00e9\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}